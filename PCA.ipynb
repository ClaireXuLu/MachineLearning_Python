{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "## 1. 算法原理\n",
    "\n",
    "pca是特征降维的经典算法之一，特征降维直白来说就是降低样本的特征维度同时又不希望丢失过多信息。如何将这个目标转换为数学问题。可以从两个角度出发：\n",
    "\n",
    "一是使得降维后的向量和原始向量之间的误差和最小化，基于最小投影距离：\n",
    "$$\n",
    "\\mathop{argmin}_W　 (-tr(W^TXX^TW))  　 s.t. W^TW=I\n",
    "$$\n",
    "二是使得样本点经过投影后还能区分的开，方差大，基于最大投影方差：\n",
    "$$\n",
    "\\mathop{argmax}_W (tr(W^TXX^TW)　s.t. W^TW=I\n",
    "$$\n",
    "这里W是 n*k维的向量，n是初始向量维度, k是降维后的维度。具体的推导其实我还是不很清楚，先了解一个算法实现，之后需要我再去仔细理解推导，可以参考这篇博客：[PCA原理总结][https://www.cnblogs.com/pinard/p/6239403.html] \n",
    "\n",
    "## 2. 算法流程总结\n",
    "\n",
    "1. 对所有样本去中心化 $x^{(i)} = x^{(i)} - \\frac{1}{m}\\sum_{j=1}^m{x^{(j)}}$ \n",
    "2. 计算样本的协方差矩阵$X^TX$ \n",
    "3. 对协方差矩阵$X^TX$ 进行特征值分解\n",
    "4. 取出最大的k个特征值对应的特征向量\n",
    "5. 对每个样本进行特征转换 $z^{(i)} = W^Tx^{(i)}$ \n",
    "\n",
    "   或者有时降维并不指定k，而是指定主成分的比重阈值t，选取前k大的特征值满足：\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{i=1}^n\\lambda_i} \\ge t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, k):\n",
    "    \"\"\"dimensionality reduction method\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): m * n, m samples, n dims feature\n",
    "        k (integer): hope to dimension's final ndim number\n",
    "\n",
    "    Returns:\n",
    "        Z (ndarray): m * k\n",
    "    \"\"\"\n",
    "    # decentralization\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    # calculate covariance matrix of X\n",
    "    X_cov = np.cov(X, rowvar=False)\n",
    "    # eigenvalue decomposition of the covariance matrix\n",
    "    eigvalues, eigvectors = np.linalg.eig(X_cov)\n",
    "    # pick first-k eigvector as W\n",
    "    max_eigvalue_index = np.argsort(-eigvalues)[:k]\n",
    "    W = eigvectors[:, max_eigvalue_index]\n",
    "\n",
    "    Z = X @ W\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.82797019]\n",
      " [ 1.77758033]\n",
      " [-0.99219749]\n",
      " [-0.27421042]\n",
      " [-1.67580142]\n",
      " [-0.9129491 ]\n",
      " [ 0.09910944]\n",
      " [ 1.14457216]\n",
      " [ 0.43804614]\n",
      " [ 1.22382056]]\n"
     ]
    }
   ],
   "source": [
    "X = [[2.5, 2.4],\n",
    "     [0.5, 0.7],\n",
    "     [2.2, 2.9],\n",
    "     [1.9, 2.2],\n",
    "     [3.1, 3.0],\n",
    "     [2.3, 2.7],\n",
    "     [2, 1.6],\n",
    "     [1, 1.1],\n",
    "     [1.5, 1.6],\n",
    "     [1.1, 0.9]]\n",
    "Z = pca(X, 1)\n",
    "print(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('myEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e38ba9a5944a3421f77c1c2862354ade0dcdf22eb57eefe299a6ebec0eb6e7ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
